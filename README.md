# ğŸ§¾ ModelSpec Harness (Enterprise)

Treat policies like code. Test them across models. Ship with confidence.

**ModelSpec Harness** is a spec-driven, multi-model compliance harness for LLM workflows. Define a **policy / SOP / specification** once, run the same **test cases** across multiple **OpenAI + Groq** models, and export reproducible, audit-ready artifacts.

---

## ğŸš€ Live demo (Vercel)

Demo: `https://modelspec-harness-wiqi.vercel.app/`

**Important note about downloads on the Vercel demo:** Vercel functions run in a **stateless** environment, so the demo does **not** persist artifacts to disk.  
Instead, the API returns all artifacts **inline** in the `artifacts_inline` payload (HTML, CSV, JSONL, PDF), and the UI downloads from that inline data.

âœ… **Local runs are unaffected** â€” when you run locally, artifacts are persisted under `./runs/<runId>/` and downloads work normally from disk.

---

## ğŸŒŸ Key benefits

### For engineering teams
- Predictable, spec-based behavior testing instead of prompt guesswork
- Deterministic checks that are stable and CI-safe
- Easy model portability and side-by-side benchmarking
- Clear reliability, latency, and cost tradeoffs

### For product and operations
- Higher confidence at launch and after model upgrades
- Faster iteration with reusable test cases
- Early detection of policy drift and over-promising

### For compliance and governance
- Audit-ready HTML, PDF, CSV, and JSONL artifacts
- Full reproducibility with local run history
- Clear evidence and rule-level explanations
- Deterministic-first design with optional LLM judgment

---

## ğŸ“¦ What you get per run

- âœ… PASS/FAIL per case and per model
- ğŸš¨ Violations with severity (Critical, High, Medium, Low)
- â±ï¸ Latency and token usage
- ğŸ’¸ Cost estimates (rate-card configurable)
- ğŸ“„ Shareable HTML reports
- ğŸ§¾ Machine-readable evidence (JSONL)

Built by **Wiqi Lee** â€” ğ•: **[@wiqi_lee](https://x.com/wiqi_lee)**  
Provider routing: **OpenAI** + **Groq**

> âš ï¸ Disclaimer: This project is **not affiliated with or endorsed by OpenAI or Groq**.

---

## ğŸ¯ Why this exists

In real production workflows (support, operations, compliance, internal tools), failures are rarely about model intelligence. They are usually about:

- the model not following policy or SOP
- over-promising or unsafe commitments
- behavioral drift across prompts, temperatures, or providers
- lack of auditability and reproducibility
- missing artifacts for review and sign-off

ModelSpec Harness provides a **reproducible compliance gate** you can run locally or in CI.

---

## ğŸ§  How evaluation works

ModelSpec Harness supports two complementary evaluation modes.

### 1) Deterministic checks (local-only)
Fast, repeatable, and CI-friendly rule validation, including:
- required fields
- forbidden phrases
- max length constraints
- pattern-based rules

Use this mode for automated gating and baseline enforcement.

### 2) LLM auditor (strict JSON verdict)
An optional second model call for nuanced judgment, such as:
- implied promises
- uncertainty handling
- tone and subtle policy violations

Use this mode for deeper reviews and audit narratives.

---

## ğŸ“ Run artifacts (audit-grade)

Each run is stored locally under:

`./runs/<runId>/`

Artifacts include:
- `meta.json` â€” run metadata and settings
- `violations.jsonl` â€” evidence and rule hits
- `compliance_table.csv` â€” spreadsheet-friendly summary
- `report.html` â€” shareable review report
- `report.pdf` â€” portable audit report

---

## ğŸ’¸ Cost estimation

Cost is an **estimate**, calculated from token usage multiplied by your configured rate card.

- Useful for relative comparisons across models
- Helpful for planning and tradeoff analysis
- Not intended to match provider invoices

---

## âš¡ Quickstart

### 1) Install
```bash
npm install
```

### 2) Configure environment
Create an env file and add provider keys.

```bash
cp .env.example .env
```

Then set one or both keys:

```bash
OPENAI_API_KEY=sk_your_key_here
GROQ_API_KEY=gsk_your_key_here
```

âœ… You may configure **only one provider** if needed.

### 3) Run
```bash
npm run dev
```

Open: `http://localhost:3000`

---

## ğŸ”Œ Providers

- **OpenAI**: set `OPENAI_API_KEY`
- **Groq**: set `GROQ_API_KEY`

The UI uses **provider-prefixed model IDs**, for example:
- `openai:gpt-4o-mini`
- `groq:llama-3.1-70b-versatile`

---

## ğŸ§© Specs (YAML)

A spec defines compliance rules such as:
- required fields
- forbidden phrases
- uncertainty handling
- max length constraints

The UI validates YAML before execution.

---

## ğŸ§ª CI-friendly usage

Recommended CI pattern:
- Use **Deterministic (local-only)** mode for stable gating
- Archive `./runs/<runId>/report.html`, `report.pdf`, `compliance_table.csv`, `violations.jsonl`, and `meta.json` as build artifacts

Typical commands:
```bash
npm run lint
npm run build
```

---

## ğŸ—ºï¸ Project structure

```text
app/
  api/          # run endpoints and artifact downloads
  page.tsx      # web UI
lib/
  evaluator.ts  # rule engine and optional auditor
  reporters.ts  # CSV, HTML, and PDF exports
  storage.ts    # run persistence
  providers/    # OpenAI and Groq clients
runs/           # generated artifacts (gitignored)
```

---

## ğŸ§± Tech stack

- Next.js (App Router) and TypeScript
- Tailwind CSS
- p-limit for concurrency
- pdfkit for PDF generation
- js-yaml and zod for specs and validation

---

## ğŸ” Security and data handling

- All artifacts are generated and stored locally under `./runs`
- Specs and cases should not contain secrets or API keys
- Reports may include prompts and model outputs; handle accordingly
- Suitable for internal and controlled environments

---

## ğŸ“œ License

- MIT â€” see [LICENSE](LICENSE).
